{"paragraphs":[{"text":"%md\n# Notebook to read CSV files, and load data into Vector or VectorH using the Spark-Vector connector.\n\nThis Notebook illustrates how to use Scala and Spark to read a folder full of CSV files (compressed or uncompressed), then create tables in Vector based on those files. \n\nIt is intended to act as a source of useful code fragments that can be adapted to suit the needs of a particular test scenario. It is not intended to be production quality.\n\nIt will also take in incremental loads to each table, using a user-specified timestamp column to coalesce rows to leave only the most recent behind.\nThere is a static List defined below that declares tables as either Fact or Dimensions. Dimension tables are deleted and created afresh with every run.\nFact tables have the coalesce logic applied based on the timestamp, mentioned above. The name of the column to use as the timestamp is specific in the code below.\n\n##Assumptions\nIt assumes that the Spark-Vector connector is already available to Zeppelin, as is the Databricks Spark CSV library. Both of these JARs should be copied into Zeppelin's `interpreter/spark/dep` folder.\nIt assumes the table name from the file name and uses Spark CSV to infer data types for each column from the data. \nIt assumes that there is a header row present in every CSV file.\n\n## Handy references for Scala and Spark documentation\nhttp://otfried.org/scala/index.html\nhttp://spark.apache.org/docs/latest/sql-programming-guide.html\nhttp://spark.apache.org/docs/latest/sql-programming-guide.html#dataframe-operations\nhttps://databricks.gitbooks.io/databricks-spark-reference-applications/content/logs_analyzer/chapter2/s3.html\nhttp://www.sparktutorials.net\n\n","dateUpdated":"Jul 11, 2016 4:50:33 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467729601148_-113235065","id":"20160705-154001_1246550092","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Notebook to read CSV files, and load data into Vector or VectorH using the Spark-Vector connector.</h1>\n<p>This Notebook illustrates how to use Scala and Spark to read a folder full of CSV files (compressed or uncompressed), then create tables in Vector based on those files.</p>\n<p>It is intended to act as a source of useful code fragments that can be adapted to suit the needs of a particular test scenario. It is not intended to be production quality.</p>\n<p>It will also take in incremental loads to each table, using a user-specified timestamp column to coalesce rows to leave only the most recent behind.\n<br  />There is a static List defined below that declares tables as either Fact or Dimensions. Dimension tables are deleted and created afresh with every run.\n<br  />Fact tables have the coalesce logic applied based on the timestamp, mentioned above. The name of the column to use as the timestamp is specific in the code below.</p>\n<h2>Assumptions</h2>\n<p>It assumes that the Spark-Vector connector is already available to Zeppelin, as is the Databricks Spark CSV library. Both of these JARs should be copied into Zeppelin's <code>interpreter/spark/dep</code> folder.\n<br  />It assumes the table name from the file name and uses Spark CSV to infer data types for each column from the data.\n<br  />It assumes that there is a header row present in every CSV file.</p>\n<h2>Handy references for Scala and Spark documentation</h2>\n<p>http://otfried.org/scala/index.html\n<br  />http://spark.apache.org/docs/latest/sql-programming-guide.html\n<br  />http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframe-operations\n<br  />https://databricks.gitbooks.io/databricks-spark-reference-applications/content/logs_analyzer/chapter2/s3.html\n<br  />http://www.sparktutorials.net</p>\n"},"dateCreated":"Jul 5, 2016 3:40:01 PM","dateStarted":"Jul 11, 2016 4:50:33 PM","dateFinished":"Jul 11, 2016 4:50:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:32"},{"text":"%md\n## First, get a list of all CSV files (including compressed ones) in the nominated directory\n\n### Example of reading from Amazon S3\n\nThe only change to the CSV logic below for S3 is to change the file's path to an S3 path.\nAlso, you will need to have your S3 access credentials set up so you have permissions to access the files.\n\nYou can do this by calling methods on the Spark Context, like this:\n\n    sc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", YOUR_ACCESS_KEY)\n    sc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", YOUR_SECRET_KEY)\n\n    S3 path format looks like this: \"s3n://YOUR_BUCKET_NAME/YOUR_CSV.csv\"","dateUpdated":"Jul 11, 2016 4:50:33 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467984856060_-79142543","id":"20160708-143416_723909094","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>First, get a list of all CSV files (including compressed ones) in the nominated directory</h2>\n<h3>Example of reading from Amazon S3</h3>\n<p>The only change to the CSV logic below for S3 is to change the file's path to an S3 path.\n<br  />Also, you will need to have your S3 access credentials set up so you have permissions to access the files.</p>\n<p>You can do this by calling methods on the Spark Context, like this:</p>\n<pre><code>sc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", YOUR_ACCESS_KEY)\nsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", YOUR_SECRET_KEY)\n\nS3 path format looks like this: \"s3n://YOUR_BUCKET_NAME/YOUR_CSV.csv\"\n</code></pre>\n"},"dateCreated":"Jul 8, 2016 2:34:16 PM","dateStarted":"Jul 11, 2016 4:50:33 PM","dateFinished":"Jul 11, 2016 4:50:34 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33","focus":true},{"text":"import java.io.File\r\n\r\n// Function to allow us to filter files in a folder by their suffix\r\ndef getListOfFiles(dir: String, extensions: List[String]) : List[File] = {  \r\n    val d = new File(dir)  \r\n    if (d.exists && d.isDirectory && extensions != Nil) {    \r\n        d.listFiles.filter(_.isFile).toList.filter { file => extensions.exists(file.getName.endsWith(_)) }\r\n    } else if (d.exists && d.isDirectory) {\r\n        d.listFiles.filter(_.isFile).toList \r\n    } else    {    List[File]()  }\r\n}\r\n\r\n// The suffix is a List in case we want multiple types, but we only look at CSVs and compressed CSVs at this point.\r\nval files = getListOfFiles(\"/Temp/Sample data\", List(\".csv\", \".csv.gz\"))\r\nfiles.foreach(println)","dateUpdated":"Jul 19, 2016 10:24:07 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467709126816_-1776040572","id":"20160705-095846_1983218460","result":{"code":"SUCCESS","type":"TEXT","msg":"import java.io.File\ngetListOfFiles: (dir: String, extensions: List[String])List[java.io.File]\nfiles: List[java.io.File] = List(\\Temp\\Sample data\\Sports_Slip_Result.csv)\n\\Temp\\Sample data\\Sports_Slip_Result.csv\r\n"},"dateCreated":"Jul 5, 2016 9:58:46 AM","dateStarted":"Jul 19, 2016 10:24:07 AM","dateFinished":"Jul 19, 2016 10:26:01 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:34","focus":true},{"text":"%md\n### Convert this list of file names to a set of table names\nTake the last element of the path, then strip the extension from that, and lowercase the name.","dateUpdated":"Jul 11, 2016 4:50:34 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467729834021_293599409","id":"20160705-154354_338230026","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Convert this list of file names to a set of table names</h3>\n<p>Take the last element of the path, then strip the extension from that, and lowercase the name.</p>\n"},"dateCreated":"Jul 5, 2016 3:43:54 PM","dateStarted":"Jul 11, 2016 4:50:34 PM","dateFinished":"Jul 11, 2016 4:50:35 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:35"},{"text":"// Need to convert full path of file name into a table name\r\n// Test one element by itself first, to make sure logic works\r\n// Split the pathname on backslashes, then get the last of these, and drop the '.csv' part from the end\r\nval fragments = files(0).toString.split(\"\"\"\\\\\"\"\")\r\nprintln(fragments.last.replace(\".csv\", \"\").replace(\".gz\", \"\")) // Make sure that chopping off the extension works according to plan\r\n\r\n//Get the table names as a List by using the map function (which returns a List, not a Map)\r\nval temp_tables = files.map(s => s.toString.split(\"\"\"\\\\\"\"\").last.toLowerCase.replace(\".csv\", \"\").replace(\".gz\", \"\"))\r\ntemp_tables.foreach(println)","dateUpdated":"Jul 19, 2016 10:38:20 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467709160329_1741162018","id":"20160705-095920_1769959888","result":{"code":"SUCCESS","type":"TEXT","msg":"fragments: Array[String] = Array(\"\", Temp, Sample data, Sports_Slip_Result.csv)\nSports_Slip_Result\r\ntemp_tables: List[String] = List(sports_slip_result)\nsports_slip_result\r\n"},"dateCreated":"Jul 5, 2016 9:59:20 AM","dateStarted":"Jul 19, 2016 10:38:20 AM","dateFinished":"Jul 19, 2016 10:38:22 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:36","focus":true},{"text":"%md\n### Next, get a Map containing a list of all the tables and their properties that we need to process","dateUpdated":"Jul 11, 2016 4:50:34 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467728606291_1871488062","id":"20160705-152326_956135833","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Next, get a Map containing a list of all the tables and their properties that we need to process</h3>\n"},"dateCreated":"Jul 5, 2016 3:23:26 PM","dateStarted":"Jul 11, 2016 4:50:35 PM","dateFinished":"Jul 11, 2016 4:50:36 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37"},{"text":"// Specify the names of the 'id' columns for each table.\n// This assumes that each table has a single column that is it's key. Would need a different structure if using a compound key.\n// If you have a really big schema, you might want to find a way to extract this information automatically from the data rather than type them in manually.\n// Map data type is a (key, value) pair structure with the -> operator linking these together. In this case, both are just String types.\n\nval keys = Map (\"sport\"                 -> \"wh_sports_id\", \n                \"sports_slip_result\"    -> \"wh_slip_id\", \n                \"sports_system_slip_type\" -> \"wh_system_slip_type_id\"\n                )\n\n// Statically declare a List of the fact tables. Everything not listed is a Dimension.\n// Later on, we use this to truncate and reload dimension tables from scratch with each data drop, vs doing 'upsert' logic for fact tables.\n\nval facts = List(\"sports_slip_result\")\n\n// Table class stores the pathname to the load file, plus the name of the key column, plus a Boolean about whether it's a fact table or not.\n// using a case class to make it easier to refer to members later, rather than an anonymous Tuple.\n\ncase class Table (fileName: String, key: String, fact: Boolean)\n\n// Declare the type for the Map of tables where the key is the table name, and the value is the other details, held in the Table object.\n// Still just a (key, value) Map type, but this time the Key is a String and the Value is of Table type.\n\nvar tables = Map[String, Table]()\n\n// Loop round to process each file that we have found.\n// Create a Map of tables, keyed by table name, with a Table case class as the value - of which we populate the filename, plus the table's key column, plus the 'Fact or not' boolean\n\nfor (file <- files) {\n    val table_name = file.toString.split(\"\"\"\\\\\"\"\").last.toLowerCase.replace(\".csv\", \"\").replace(\".gz\", \"\")\n    tables += (table_name -> Table(file.toString,                     // Path name to the table's data file\n                                   keys(table_name),                  // get the value stored for the table_name key, if there is one. Keys is a Map so we can just get the Value using the table_name as the Map's Key.\n                                   facts.exists(s => s == table_name) // Not a Map, so we use 'exists' to return a Boolean if table_name is in the facts List\n                                   )\n               )\n}\n\ntables.foreach(println) // Debug check to make sure we are getting everything we need","dateUpdated":"Jul 19, 2016 10:39:30 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467714281249_-32208081","id":"20160705-112441_194309533","result":{"code":"SUCCESS","type":"TEXT","msg":"keys: scala.collection.immutable.Map[String,String] = Map(sport -> wh_sports_id, sports_slip_result -> wh_slip_id, sports_system_slip_type -> wh_system_slip_type_id)\nfacts: List[String] = List(sports_slip_result)\ndefined class Table\ntables: scala.collection.immutable.Map[String,Table] = Map()\n(sports_slip_result,Table(\\Temp\\Sample data\\Sports_Slip_Result.csv,wh_slip_id,true))\r\n"},"dateCreated":"Jul 5, 2016 11:24:41 AM","dateStarted":"Jul 19, 2016 10:39:30 AM","dateFinished":"Jul 19, 2016 10:39:32 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38","focus":true},{"text":"%md\n### Once we have the list of tables and properties, then we can load the data.\n### This Paragraph is the SparkSQL version of how to do this.\n#### After registering, we create a table in Vector that matches the Spark version.\n#### And then load the data from one to the other.\n#### Note that this approach assumes that each dimension table is supplied in full every day, so we simply delete the previous data and reload every time.\n#### You would have to change this logic if that doesn't apply to your situation.","dateUpdated":"Jul 11, 2016 4:50:37 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467729938456_-576966403","id":"20160705-154538_1019687562","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Once we have the list of tables and properties, then we can load the data.</h3>\n<h3>This Paragraph is the SparkSQL version of how to do this.</h3>\n<h4>After registering, we create a table in Vector that matches the Spark version.</h4>\n<h4>And then load the data from one to the other.</h4>\n<h4>Note that this approach assumes that each dimension table is supplied in full every day, so we simply delete the previous data and reload every time.</h4>\n<h4>You would have to change this logic if that doesn't apply to your situation.</h4>\n"},"dateCreated":"Jul 5, 2016 3:45:38 PM","dateStarted":"Jul 11, 2016 4:50:38 PM","dateFinished":"Jul 11, 2016 4:50:38 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:39"},{"text":"import org.apache.spark.sql.SQLContext\r\nimport com.actian.spark_vector\r\nimport java.util.Calendar\r\nimport java.text.SimpleDateFormat\r\nimport sqlContext.implicits._\r\n\r\nval sqlContext = new SQLContext(sc)\r\n\r\n// Change the name of the timestamp column below to suit your needs\r\nval timestamp_col_name = \"last_upd_timestamp\"\r\n\r\n// Get current date, so we can use this in our data merging process\r\nval instant = Calendar.getInstance().getTime()\r\nval todayformat = new SimpleDateFormat(\"yyyy-MM-dd\")\r\nval today = todayformat.format(instant)\r\n\r\n// Iterate through the list of tables one at a time. Each table is a Map, so use a (key, value) tuple construct to get these values so we can refer to them in the loop\r\nfor ((name, table) <- tables) {\r\n    // Register the CSV file as a temporary table to Spark SQL\r\n    // If you are missing a header row, then you will need to specify a map of column names for the schema.\r\n    // Alternatively, you can specify the schema - including data types - from scratch. But you would need some other list of schema's for that.\r\n    \r\n    sqlContext.sql(s\"\"\"CREATE TEMPORARY TABLE csv_$name USING com.databricks.spark.csv OPTIONS \"\"\" + \r\n                    \"\"\"(path \"\"\"\" + table.fileName + \"\"\"\", header \"true\", inferSchema \"true\")\"\"\")\r\n                    \r\n    // Adjust the options in here to match your setup.\r\n    // The table definition in this form is taken from the table name referenced in the 'options' clause\r\n    \r\n    // For dimension tables, delete all the data before loading, as we reload the whole thing from scratch.\r\n    // For Fact tables, run the 'upsert' merge logic as part of the load process. \r\n    // Delete timestamps before today if a key is present in the table more than once\r\n    // Note: this upsert logic assumes only one load per day. Amend it if you need  more than one.\r\n    // Spark doesn't have a 'DELETE' statement, so we have to put Vector-specific logic into these pre- and post- options to pass them to Vector unchanged by SparkSQL.\r\n    \r\n    val delete_facts = s\"DELETE FROM $name WHERE last_upd_timestamp < DATE (‘$today’) AND ${table.key} IN (SELECT ${table.key} FROM $name WHERE $timestamp_col_name  >= DATE (‘$today’) HAVING COUNT($timestamp_col_name) > 1 )\"\r\n    \r\n    // Pre-process: Truncate dimension tables - no history required    \r\n    // Spark SQL won't understand the Modify statement, and we don't have SQL pass-through today.\r\n    // So the only way to do this is to add the 'modify' to the \"loadpreSQL\" option within the create table statement\r\n    // Or, simply drop the table and re-create it each time. Dropping the table was added to the 'loadpreSQL' clause in the above statement.\r\n    \r\n    val delete_dims  = s\"MODIFY $name TO TRUNCATED\"\r\n    \r\n    // Note that this process does not actually create the Vector table. This part needs to be done outside of Spark.\r\n    // The 'CREATE TEMPORARY' part of the syntax here just registers to Spark the table that lives inside Vector\r\n    // or else registers to SparkSQL a file of data or DataFrame as a table.\r\n    \r\n    if (table.fact) {\r\n        sqlContext.sql(s\"\"\"CREATE TEMPORARY TABLE $name USING com.actian.spark_vector.sql.DefaultSource OPTIONS \"\"\" +\r\n                    s\"\"\"(host \"localhost\", instance \"VH\", user \"actian\", password \"actian\", database \"databasename\", table \"csv_$name\", loadpostSQL \"$delete_facts\") \"\"\")\r\n                    \r\n    } else { \r\n        sqlContext.sql(s\"\"\"CREATE TEMPORARY TABLE $name USING com.actian.spark_vector.sql.DefaultSource OPTIONS \"\"\" +\r\n                    \"\"\"(host \"localhost\", instance \"VH\", user \"actian\", password \"actian\", database \"databasename\", table \"csv_$name\", loadpreSQL \"$delete_dims\") \"\"\")\r\n    }\r\n\r\n    // Copy the data from CSV to Vector table\r\n    sqlContext.sql(s\"INSERT INTO $name SELECT * FROM csv_$name\")\r\n}","dateUpdated":"Jul 19, 2016 3:03:35 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","tableHide":true,"editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467722473577_-1895904696","id":"20160705-134113_1532483368","result":{"code":"ERROR","type":"TEXT","msg":"import org.apache.spark.sql.SQLContext\nimport com.actian.spark_vector\nimport java.util.Calendar\nimport java.text.SimpleDateFormat\nimport sqlContext.implicits._\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@4e4e1db5\ntimestamp_col_name: String = last_upd_timestamp\ninstant: java.util.Date = Mon Jul 11 16:50:45 BST 2016\ntodayformat: java.text.SimpleDateFormat = java.text.SimpleDateFormat@f67a0200\ntoday: String = 2016-07-11\njava.sql.SQLNonTransientConnectionException: Communications error while establishing connection.\n\tat com.ingres.gcf.util.SqlExType.getSqlEx(SqlExType.java:97)\n\tat com.ingres.gcf.util.SqlExFactory.get(SqlExFactory.java:68)\n\tat com.ingres.gcf.util.SqlExFactory.get(SqlExFactory.java:158)\n\tat com.ingres.gcf.util.SqlExFactory.get(SqlExFactory.java:197)\n\tat com.ingres.gcf.dam.MsgIo.connect(MsgIo.java:474)\n\tat com.ingres.gcf.dam.MsgOut.connect(MsgOut.java:338)\n\tat com.ingres.gcf.dam.MsgIn.connect(MsgIn.java:365)\n\tat com.ingres.gcf.dam.MsgConn.connect(MsgConn.java:363)\n\tat com.ingres.gcf.jdbc.DrvConn.connect(DrvConn.java:824)\n\tat com.ingres.gcf.jdbc.DrvConn.<init>(DrvConn.java:507)\n\tat com.ingres.gcf.jdbc.JdbcDrv.connect(JdbcDrv.java:652)\n\tat java.sql.DriverManager.getConnection(Unknown Source)\n\tat java.sql.DriverManager.getConnection(Unknown Source)\n\tat com.actian.spark_vector.vector.VectorJDBC.<init>(VectorJDBC.scala:133)\n\tat com.actian.spark_vector.vector.VectorJDBC$$anonfun$6.apply(VectorJDBC.scala:273)\n\tat com.actian.spark_vector.vector.VectorJDBC$$anonfun$6.apply(VectorJDBC.scala:273)\n\tat resource.DefaultManagedResource.open(AbstractManagedResource.scala:106)\n\tat resource.AbstractManagedResource.acquireFor(AbstractManagedResource.scala:85)\n\tat resource.DeferredExtractableManagedResource.either(AbstractManagedResource.scala:29)\n\tat com.actian.spark_vector.util.ResourceUtil$RichExtractableManagedResource$.resolve$extension(ResourceUtil.scala:30)\n\tat com.actian.spark_vector.vector.VectorJDBC$.withJDBC(VectorJDBC.scala:273)\n\tat com.actian.spark_vector.sql.VectorRelation$.structType(VectorRelation.scala:77)\n\tat com.actian.spark_vector.sql.VectorRelation$$anonfun$schema$1.apply(VectorRelation.scala:32)\n\tat com.actian.spark_vector.sql.VectorRelation$$anonfun$schema$1.apply(VectorRelation.scala:32)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat com.actian.spark_vector.sql.VectorRelation.schema(VectorRelation.scala:32)\n\tat org.apache.spark.sql.execution.datasources.LogicalRelation.<init>(LogicalRelation.scala:37)\n\tat org.apache.spark.sql.execution.datasources.CreateTempTableUsing.run(ddl.scala:96)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)\n\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:145)\n\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:130)\n\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:52)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:817)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:91)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:63)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:109)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:63)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:106)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:108)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:110)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:112)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:114)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:116)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:118)\n\tat $iwC$$iwC$$iwC$$iwC.<init>(<console>:120)\n\tat $iwC$$iwC$$iwC.<init>(<console>:122)\n\tat $iwC$$iwC.<init>(<console>:124)\n\tat $iwC.<init>(<console>:126)\n\tat <init>(<console>:128)\n\tat .<init>(<console>:132)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.lang.reflect.Method.invoke(Unknown Source)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:713)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:678)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:671)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(Unknown Source)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.lang.Thread.run(Unknown Source)\n\n"},"dateCreated":"Jul 5, 2016 1:41:13 PM","dateStarted":"Jul 11, 2016 4:50:39 PM","dateFinished":"Jul 11, 2016 4:50:51 PM","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:40"},{"text":"%md\n### This Paragraph is the Spark API version of how to do this.\n#### Both this paragraph and the one above implement the same logic - so you only need one or the other, not both.\n#### After registering, we create a table in Vector that matches the Spark version.\n#### And then load the data from one to the other.","dateUpdated":"Jul 11, 2016 4:50:38 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467817703637_-1361340515","id":"20160706-160823_1061624603","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>This Paragraph is the Spark API version of how to do this.</h3>\n<h4>Both this paragraph and the one above implement the same logic - so you only need one or the other, not both.</h4>\n<h4>After registering, we create a table in Vector that matches the Spark version.</h4>\n<h4>And then load the data from one to the other.</h4>\n"},"dateCreated":"Jul 6, 2016 4:08:23 PM","dateStarted":"Jul 11, 2016 4:50:38 PM","dateFinished":"Jul 11, 2016 4:50:39 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41"},{"text":"import org.apache.spark.sql.SQLContext\r\nimport com.actian.spark_vector\r\nimport java.util.Calendar\r\nimport java.text.SimpleDateFormat\r\nimport sqlContext.implicits._\r\n\r\nval sqlContext = new SQLContext(sc)\r\n\r\n// Change the name of the timestamp column below to suit your needs\r\nval timestamp_col_name = \"last_upd_timestamp\"\r\n\r\n// Get current date, so we can use this in our data merging process\r\n// Alternatively, could just select the minimum date from each CSV file and use that as the marker.\r\n// Date formatting code left in to provide an example.\r\nval instant = Calendar.getInstance().getTime()\r\nval todayformat = new SimpleDateFormat(\"yyyy-MM-dd\")\r\nval today = todayformat.format(instant)\r\n\r\n// Iterate through the list of tables one at a time. Each table is a Map, so use a (key, value) tuple construct to get these values so we can refer to them in the loop\r\nfor ((name, table) <- tables) {\r\n    // Only way to execute a DELETE statement through Spark is to pass it as a postload statement via the Options, so set that up here for Fact tables.\r\n    val delete_facts     = s\"DELETE FROM $name WHERE last_upd_timestamp < DATE (‘$today’) AND $table.key IN (SELECT $table.key FROM $name WHERE $timestamp_col_name  >= DATE (‘$today’) HAVING COUNT($timestamp_col_name) > 1 )\"\r\n    val delete_dims      = s\"MODIFY $name TO TRUNCATED\"\r\n    \r\n    val vector_fact_opts = Map(\"host\"->\"localhost\", \"user\"->\"actian\", \"password\"->\"actian\", \"instance\"->\"VW\", \"database\"->\"sample\",\"table\"->name, \"loadpostSQL\"-> delete_facts)\r\n    val vector_dim_opts  = Map(\"host\"->\"localhost\", \"user\"->\"actian\", \"password\"->\"actian\", \"instance\"->\"VW\", \"database\"->\"sample\",\"table\"->name, \"preloadSQL\" -> delete_dims)\r\n    val csv_opts         = Map(\"path\" -> table.fileName, \"header\"->\"true\", \"inferSchema\"->\"true\", \"dateFormat\" -> \"dd/MM/yyyy hh:mm\")\r\n    \r\n    // Note that the inferSchema option is limited in terms of working out date/time formats. If a file has both date values (with no time) and timstamps, \r\n    // either both of these will be flagged as a timestamp but the timstamp will not have a time, or else only one will be a timestamp and the other will be flagged as a String,\r\n    // depending on whether you add hh:mm to the end of the dateFormat string or not.\r\n\r\n    // Create a DataFrame that represents the CSV file. This will load the whole CSV file into Spark memory.\r\n    // Note that this requires the external Databricks Spark CSV package.\r\n    val csv_df = sqlContext.read\r\n        .format(\"com.databricks.spark.csv\")\r\n        .options(csv_opts)\r\n        .load()\r\n\r\n    // We don't actually use it as a table in this model, but this is how you would do so\r\n    csv_df.registerTempTable(s\"csv_$name\")\r\n    println(s\"Data types inferred for table csv_$name are:\")\r\n    csv_df.printSchema()    // Show how Spark has mapped the types in the CSV file\r\n    csv_df.show()\r\n    \r\n    // csv_df.filter(df(\"age\") > 21).show()             // Example of how to print and filter a column's value in a DataFrame\r\n    // csv_df.col(\"age\").cast(\"int\")                    // Example of how to cast to a type if inferSchema doesn't work well for you\r\n    // val csv_df2 = csv_df.withColumnRenamed(\"Year\", \"orig_Year\")\r\n    // val csv_df3 = csv_df2.withColumn(\"Year\", csv_df2.col(\"org_Year\").cast(\"int\")).drop(\"orig_Year\")      // How to replace an old column with a new one, in a new DataFrame\r\n    // val csv_df4 = csv_df2.select(\"test_col\").distinct.count\r\n    // val csv_df5 = csv_df2.groupBy().agg(min(\"Year\")\r\n    \r\n    // Write the data into the VectorH table from the DataFrame\r\n    // SaveMode = Append means that the data is appended to the table, not updated.\r\n    // Note that this process does not actually create the Vector table. This part needs to be done outside of Spark.\r\n    \r\n    val this_table = if (table.fact) vector_fact_opts else vector_dim_opts\r\n    \r\n    csv_df.write\r\n        .format(\"com.actian.spark_vector.sql.DefaultSource\")\r\n        .mode(org.apache.spark.sql.SaveMode.Append)\r\n        .options(this_table)\r\n        .save()\r\n}","dateUpdated":"Jul 19, 2016 3:01:51 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467817635874_1939004378","id":"20160706-160715_439370562","result":{"code":"ERROR","type":"TEXT","msg":"import org.apache.spark.sql.SQLContext\nimport com.actian.spark_vector\nimport java.util.Calendar\nimport java.text.SimpleDateFormat\nimport sqlContext.implicits._\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@3c268c63\ntimestamp_col_name: String = last_upd_timestamp\ninstant: java.util.Date = Mon Jul 11 17:40:20 BST 2016\ntodayformat: java.text.SimpleDateFormat = java.text.SimpleDateFormat@f67a0200\ntoday: String = 2016-07-11\nData types inferred for table csv_sports_slip_result are:\r\nroot\n |-- Wh_Slip_Id: long (nullable = true)\n |-- Bet_Placed_Date_GMT: string (nullable = true)\n |-- Wh_Result_Id: integer (nullable = true)\n |-- Bet_Odds_Value: double (nullable = true)\n |-- Last_Upd_Timestamp: timestamp (nullable = true)\n\r\n+----------+-------------------+------------+--------------+--------------------+\n|Wh_Slip_Id|Bet_Placed_Date_GMT|Wh_Result_Id|Bet_Odds_Value|  Last_Upd_Timestamp|\n+----------+-------------------+------------+--------------+--------------------+\n|2207232650|         09/01/2013|   162979627|           3.2|2013-01-10 03:12:...|\n|2204687711|         05/01/2013|   162520373|           5.5|2013-01-06 02:18:...|\n|2203498779|         04/01/2013|   162282965|           2.1|2013-01-05 01:43:...|\n|2203864221|         04/01/2013|   161999135|          1.25|2013-01-05 01:43:...|\n|2202273802|         01/01/2013|   151604898|           1.5|2013-01-02 01:44:...|\n|2206409991|         08/01/2013|   162839196|           1.6|2013-01-09 01:44:...|\n|2204375883|         05/01/2013|   162001279|          1.62|2013-01-06 02:18:...|\n|2202466352|         02/01/2013|   162007545|           1.7|2013-01-03 03:22:...|\n|2204753939|         05/01/2013|   162625371|           3.4|2013-01-06 02:18:...|\n|2201423959|         31/12/2012|   161296260|          1.65|2013-01-01 02:29:...|\n|2206073476|         07/01/2013|   160923501|           3.3|2013-01-08 02:26:...|\n|2201528489|         31/12/2012|   162039358|          2.05|2013-01-01 02:29:...|\n|2204224478|         05/01/2013|   161998307|          1.75|2013-01-06 02:18:...|\n|2204915376|         05/01/2013|   162637246|          1.09|2013-01-06 02:18:...|\n|2202044677|         01/01/2013|   161978158|           1.6|2013-01-02 01:44:...|\n|2204654474|         05/01/2013|   160752731|          1.65|2013-01-06 02:18:...|\n|2203131231|         03/01/2013|   161543377|          1.95|2013-01-04 02:00:...|\n|2204821670|         05/01/2013|   162541848|          1.78|2013-01-06 02:18:...|\n|2203604745|         04/01/2013|   162453947|           2.4|2013-01-05 01:43:...|\n|2205635640|         06/01/2013|   158617525|           2.4|2013-01-07 02:44:...|\n+----------+-------------------+------------+--------------+--------------------+\nonly showing top 20 rows\n\r\ncom.actian.spark_vector.vector.VectorException: jdbc:ingres://localhost:VW7/sample: Error connecting to Vector instance\n\tat com.actian.spark_vector.vector.Vector$.getTableSchema(Vector.scala:58)\n\tat com.actian.spark_vector.vector.LoadVector$.loadVector(LoadVector.scala:53)\n\tat com.actian.spark_vector.vector.VectorOps$VectorRDDOps.loadVector(VectorOps.scala:54)\n\tat com.actian.spark_vector.sql.VectorRelation.insert(VectorRelation.scala:44)\n\tat com.actian.spark_vector.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:222)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:148)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:123)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$2.apply(<console>:86)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:109)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:86)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:129)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:131)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:133)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:135)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:137)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:139)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:141)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:143)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:145)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:147)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:149)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:151)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:153)\n\tat $iwC$$iwC$$iwC$$iwC.<init>(<console>:155)\n\tat $iwC$$iwC$$iwC.<init>(<console>:157)\n\tat $iwC$$iwC.<init>(<console>:159)\n\tat $iwC.<init>(<console>:161)\n\tat <init>(<console>:163)\n\tat .<init>(<console>:167)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.lang.reflect.Method.invoke(Unknown Source)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:713)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:678)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:671)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(Unknown Source)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.lang.Thread.run(Unknown Source)\n\n"},"dateCreated":"Jul 6, 2016 4:07:15 PM","dateStarted":"Jul 11, 2016 5:40:18 PM","dateFinished":"Jul 11, 2016 5:40:24 PM","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:42"},{"text":"%md\n    \n# Example of reading from a Parquet file and writing to a Vector table in one go. \n\nCSV files require slightly different handling because there is no 'csv' method built into 'read'.\n\nParquet files have the schema built in, and have a '.parquet' convenience method for accessing directly.\n\n    sqlContext.read.parquet(\"hdfs://hdp-client-1/user/actian/rankings-parquet\")\n               .write.format(\"com.actian.spark_vector.sql.DefaultSource\")\n               .mode(org.apache.spark.sql.SaveMode.Append)\n               .options(vector_opts)\n               .save()\n\n","dateUpdated":"Jul 11, 2016 4:50:40 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467801984334_-854367734","id":"20160706-114624_1119430897","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Example of reading from a Parquet file and writing to a Vector table in one go.</h1>\n<p>CSV files require slightly different handling because there is no 'csv' method built into 'read'.</p>\n<p>Parquet files have the schema built in, and have a '.parquet' convenience method for accessing directly.</p>\n<pre><code>sqlContext.read.parquet(\"hdfs://hdp-client-1/user/actian/rankings-parquet\")\n           .write.format(\"com.actian.spark_vector.sql.DefaultSource\")\n           .mode(org.apache.spark.sql.SaveMode.Append)\n           .options(vector_opts)\n           .save()\n</code></pre>\n"},"dateCreated":"Jul 6, 2016 11:46:24 AM","dateStarted":"Jul 11, 2016 4:50:40 PM","dateFinished":"Jul 11, 2016 4:50:41 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:43"},{"text":"%md\n# Example of how to read directly from a database using JDBC.\n\nNote that the JDBC driver needs to be installed/copied to every node of the cluster to that each Spark executor can access it directly. e.g. \n\n    SPARK_CLASSPATH=postgresql-9.3-1102-jdbc41.jar bin/spark-shell\n\nOther options include:\n    Property    Name\tMeaning\n    url\t        The JDBC URL to connect to.\n    dbtable\t    The JDBC table that should be read. Note that anything that is valid in a FROM clause of a SQL query can be used. For example, instead of a full table you could also use a subquery in parentheses.\n    driver\t    The class name of the JDBC driver to use to connect to this URL.\n    partitionColumn, \n    lowerBound, \n    upperBound, \n    numPartitions\tThese options must all be specified if any of them is specified. They describe how to partition the table when reading in parallel from multiple workers. partitionColumn must be a numeric column from the table in question. Notice that lowerBound and upperBound are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned.\n    fetchSize\tThe JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows).\n\n\n    val jdbcDF = sqlContext.read.format(\"jdbc\")\n                                .options( Map (\"url\"       -> \"jdbc:postgresql:dbserver\", \n                                               \"dbtable\"   -> \"schema.tablename\",\n                                               \"fetchSize\" -> \"1000\" ) )\n                                .load()","dateUpdated":"Jul 11, 2016 4:50:40 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467969678328_-107000553","id":"20160708-102118_456608129","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Example of how to read directly from a database using JDBC.</h1>\n<p>Note that the JDBC driver needs to be installed/copied to every node of the cluster to that each Spark executor can access it directly. e.g.</p>\n<pre><code>SPARK_CLASSPATH=postgresql-9.3-1102-jdbc41.jar bin/spark-shell\n</code></pre>\n<p>Other options include:</p>\n<pre><code>Property    Name    Meaning\nurl         The JDBC URL to connect to.\ndbtable     The JDBC table that should be read. Note that anything that is valid in a FROM clause of a SQL query can be used. For example, instead of a full table you could also use a subquery in parentheses.\ndriver      The class name of the JDBC driver to use to connect to this URL.\npartitionColumn, \nlowerBound, \nupperBound, \nnumPartitions   These options must all be specified if any of them is specified. They describe how to partition the table when reading in parallel from multiple workers. partitionColumn must be a numeric column from the table in question. Notice that lowerBound and upperBound are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned.\nfetchSize   The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows).\n\n\nval jdbcDF = sqlContext.read.format(\"jdbc\")\n                            .options( Map (\"url\"       -&gt; \"jdbc:postgresql:dbserver\", \n                                           \"dbtable\"   -&gt; \"schema.tablename\",\n                                           \"fetchSize\" -&gt; \"1000\" ) )\n                            .load()\n</code></pre>\n"},"dateCreated":"Jul 8, 2016 10:21:18 AM","dateStarted":"Jul 11, 2016 4:50:41 PM","dateFinished":"Jul 11, 2016 4:50:42 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:44"},{"text":"%md\n# Example of how to read directly from Hive","dateUpdated":"Jul 11, 2016 4:50:40 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467969730451_1348545052","id":"20160708-102210_760143701","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Example of how to read directly from Hive</h1>\n"},"dateCreated":"Jul 8, 2016 10:22:10 AM","dateStarted":"Jul 11, 2016 4:50:42 PM","dateFinished":"Jul 11, 2016 4:50:42 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:45"},{"text":"import org.apache.spark.sql.hive.HiveContext\r\nimport org.apache.spark.sql.types._\r\nimport org.apache.spark.{SparkContext, SparkConf}\r\n \r\nval conf = new SparkConf()\r\n\r\nconf.setAppName(\"Sparky\")\r\nconf.set(\"spark.eventLog.enabled\", \"true\")\r\nconf.set(\"spark.eventLog.dir\", \"/tmp\")\r\n\r\nval sc = new SparkContext(conf)\r\n\r\n// Spark has a Hive-specific context to allow use of the Hive-specific SQL syntax. Set this up and use it next.\r\nval hc = new HiveContext(sc)\r\n\r\n// Create a Spark DataFrame based on a specific set of selection criteria, for one particular month within a year.\r\n// Could create a range here and iterate around these easily enough too.\r\n\r\n// Some examples of Hive functions and column transformation activities included below\r\n// Can also do this sort of thing on the DataFrame after it's been loaded as well - depends on what and where you want the logic to be executed\r\nval dfHive = hc.sql(\r\n  \"\"\"SELECT\r\n        CASE WHEN TRIM(day_id)='' THEN '9999-12-31' else COALESCE(day_id, '9999-12-31') end as day_id,\r\n        CASE WHEN TRIM(start_timestamp_local)='' THEN '9999-12-31 00:00:00' else COALESCE(start_timestamp_local, '9999-12-31 00:00:00') end as start_timestamp_local,\r\n        CASE WHEN TRIM(end_timestamp_local)  ='' THEN '9999-12-31 00:00:00' else COALESCE(end_timestamp_local,   '9999-12-31 00:00:00') end as end_timestamp_local,\r\n        COALESCE(company_id,' ') as company_id\r\n    FROM schema_name.hive_table \r\n    WHERE month(day_id)  = '05'  \r\n    AND   year(day_id)   = '2015' \r\n    AND   listing_day_id <> '${hivevar:start_date}'\r\n  \"\"\");\r\n\r\n    dfHive.registerTempTable(\"hive_table\");\r\n     \r\n    sqlContext.sql(\"\"\"CREATE TEMPORARY TABLE vh_table\r\n    USING com.actian.spark_vector.sql.DefaultSource\r\n    OPTIONS (\r\n    host     \"hostname.sys.domain.net\",\r\n    instance \"VH\",\r\n    database \"prod-dbname\",\r\n    table    \"table_2015_05\"\r\n  )\"\"\");\r\n     \r\n     sqlContext.sql(\"INSERT INTO table vh_table SELECT * FROM hive_table\");\r\n   }\r\n}\r\n","dateUpdated":"Jul 11, 2016 4:50:41 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467990129133_-1307060476","id":"20160708-160209_1179146887","result":{"code":"ERROR","type":"TEXT","msg":"import org.apache.spark.sql.hive.HiveContext\nimport org.apache.spark.sql.types._\nimport org.apache.spark.{SparkContext, SparkConf}\nconf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@234687bd\nres65: org.apache.spark.SparkConf = org.apache.spark.SparkConf@234687bd\nres66: org.apache.spark.SparkConf = org.apache.spark.SparkConf@234687bd\nres67: org.apache.spark.SparkConf = org.apache.spark.SparkConf@234687bd\norg.apache.spark.SparkException: A master URL must be set in your configuration\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:401)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:63)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:68)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:70)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:72)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:74)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:76)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:78)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:80)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:82)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:84)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:86)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:88)\n\tat $iwC$$iwC$$iwC$$iwC.<init>(<console>:90)\n\tat $iwC$$iwC$$iwC.<init>(<console>:92)\n\tat $iwC$$iwC.<init>(<console>:94)\n\tat $iwC.<init>(<console>:96)\n\tat <init>(<console>:98)\n\tat .<init>(<console>:102)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.lang.reflect.Method.invoke(Unknown Source)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:713)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:678)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:671)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n\tat java.util.concurrent.FutureTask.run(Unknown Source)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(Unknown Source)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.lang.Thread.run(Unknown Source)\n\n"},"dateCreated":"Jul 8, 2016 4:02:09 PM","dateStarted":"Jul 11, 2016 4:50:52 PM","dateFinished":"Jul 11, 2016 4:51:00 PM","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:46"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1468252241203_2045622794","id":"20160711-165041_740594067","dateCreated":"Jul 11, 2016 4:50:41 PM","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:47"}],"name":"Loading Vector from CSV files with Spark","id":"2BPWYVWFA","angularObjects":{"2BA7S2E48":[],"2BC2RDM3Z":[],"2B9PNW7QW":[],"2BAGXS8NV":[],"2B9MGWSGW":[],"2BA2GWURV":[],"2BCFANQAD":[],"2BBY1VZKN":[],"2B8T3BBNE":[],"2BBE56WMD":[],"2BATDWABB":[],"2BB5CMJNN":[],"2BAMR2KM4":[],"2B9EVBEED":[]},"config":{"looknfeel":"default"},"info":{}}